"""My_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YazP6iPrW7A2edl5pqCN6IO2uxDpbBWw
"""

import pandas as pd
import numpy
import plotly.express as px
# import seaborn as sns
pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 20000)

df = pd.read_csv(r'C:\Users\HP\Downloads\Project_folder\fraud test.csv')

df = df.sort_values(by='is_fraud', ascending=False)

df = df.head(20000)

df.shape

df['is_fraud'].value_counts()

print(df.head(1))

# from the head of the data we see that there are some irrelevant columns like
# the one contains the numbers of each raw so we remove this column.
# also we do not need the collums of the first and last name and the dob
#because we can't predict a fraud based on someone's name or date of birth
# df = df.drop(columns=['first','dob','last','Unnamed: 0','trans_date_trans_time','street','city_pop','unix_time','cc_num','trans_num'])
# df.drop
#the transaction time and number are irrelevant to our model because they are unique to each
# transaction and including them can increase the risk of Overfitting

# from datetime import datetime

# # datetime.now()
# pd.Timestamp.now()

df['dob'] = pd.to_datetime(df['dob'], format='mixed')

df['age'] = (pd.Timestamp.now() - df['dob']).dt.days / 365

df.shape

df.duplicated().sum()

# Display basic statistics of the numerical features
print("\nBasic statistics of numerical features:")
print(df.describe())

# Display information about the dataset including the data types and number of non-null values
print("\nInformation about the dataset:")
print(df.info())

# Check for missing values
print("\nMissing values:")
print(df.isnull().sum())

df.head(2)

# Check the unique values in each categorical feature
print("Unique values in categorical features:")
categorical_features = df.select_dtypes(include=['object'])
for column in categorical_features.columns:
    print(f"{column}: {df[column].unique()}")
    print("")

# The classes are heavily skewed we need to solve this issue later.
print('No Frauds', round(df['is_fraud'].value_counts()[0]/len(df) * 100,2), '% of the dataset')
print('Frauds', round(df['is_fraud'].value_counts()[1]/len(df) * 100,2), '% of the dataset')

#here we Notice how imbalanced is our original dataset
#Most of the transactions are non-fraud. If we use this dataframe as the
#base for our predictive models and analysis we might get a
#lot of errors and our algorithms will probably overfit since
#it will "assume" that most transactions are not fraud. But we
#don't want our model to assume, we want our model to detect
#patterns that give signs of fraud.

df['is_fraud'].value_counts()

df['is_fraud'].value_counts().plot(kind = 'bar')

# df.head()

# df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])
# df['year'] = df['trans_date_trans_time'].dt.year
# df['month'] = df['trans_date_trans_time'].dt.month
# df['day'] = df['trans_date_trans_time'].dt.day
# df['hour'] = df['trans_date_trans_time'].dt.hour
# df['minute'] = df['trans_date_trans_time'].dt.minute
# df['second'] = df['trans_date_trans_time'].dt.second
# df['day_of_week'] = df['trans_date_trans_time'].dt.dayofweek
# df['day_of_year'] = df['trans_date_trans_time'].dt.dayofyear
# df['weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)
# df['quarter'] = df['trans_date_trans_time'].dt.quarter

df.head(1)

# df.rename(columns={'year':'year_of_trans'}, inplace=True)
# df.rename(columns={'month':'month_of_trans'}, inplace=True)
# df.rename(columns={'day':'day_of_trans'}, inplace=True)
# df.rename(columns={'hour':'hour_of_trans'}, inplace=True)
# df.rename(columns={'minute':'minute_of_trans'}, inplace=True)
# df.rename(columns={'second':'second_of_trans'}, inplace=True)
# df.rename(columns={'day_of_week':'day_of_week_trans'}, inplace=True)
# df.rename(columns={'day_of_year':'day_of_year_trans'}, inplace=True)
# df.rename(columns={'weekend':'weekend_of_trans'}, inplace=True)
# df.rename(columns={'quarter':'quarter_of_trans'}, inplace=True)

# df_cat = df.select_dtypes(include='object')
# df_num = df.select_dtypes(exclude='object')
# df_cat.head()

# df_num

# px.pie(df, names='gender', values='is_fraud', title='Fraud Distribution')

# px.histogram(df, x='age', y = 'is_fraud', title='Fraud Distribution')

# df.head(1)

import numpy as np
from scipy.stats import zscore  # Import the zscore function

# np.abs(zscore(df['amt']))

from scipy import stats
import numpy as np

# Calculate z-scores for 'amt' column only
# z_scores_amt = stats.zscore(df['amt'].select_dtypes(include=np.number))

# Filter the DataFrame, keeping rows where 'amt' z-score is less than 3
# df_no_outliers = df[(z_scores_amt < 1)]

# from scipy import stats
# import numpy as np

# # Calculate z-scores for all numeric features
# z_scores = stats.zscore(df.select_dtypes(include=np.number))

# # Filter DataFrame, keeping rows where absolute z-scores are less than 3 for all numeric columns
# df_no_outliers = df[(np.abs(z_scores) < 3).all(axis=1)]

# amount of fraud classes 2145 rows.
fraud_df = df.loc[df['is_fraud'] == 1][:2145]
non_fraud_df = df.loc[df['is_fraud'] == 0][2145: 2145+2145]

normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

# Shuffle dataframe rows
df = normal_distributed_df

df.head()

import plotly.express as px
# import seaborn as sns
import matplotlib.pyplot as plt
print('Distribution of the Classes in the subsample dataset')
print(df['is_fraud'].value_counts()/len(df))

# # fig = sns.countplot(x='is_fraud', data=new_df) # Create the seaborn plot
# fig.set_title('Equally Distributed Classes', fontsize=14) # Set the title on the seaborn plot
# plt.show() # Display the seaborn plot

df.head(1)

df = df.drop(columns=['first','dob','last','zip','Unnamed: 0','trans_date_trans_time','street','city_pop','unix_time','cc_num','trans_num'])

df.head(1)

df.to_csv('df.csv_to_cleaned', index=False)

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
label_encoder = LabelEncoder()
print(df.head(1))
X = df.drop('is_fraud', axis=1)
y = df['is_fraud']
print(X.columns.tolist())
df.select_dtypes('object').columns.tolist()

merchants_encoder = LabelEncoder()
df['merchant'] = merchants_encoder.fit_transform(df['merchant'])

category_encoder = LabelEncoder()
df['category'] = category_encoder.fit_transform(df['category'])

gender_encoder = LabelEncoder()
df['gender'] = gender_encoder.fit_transform(df['gender'])

city_encoder = LabelEncoder()
df['city'] = city_encoder.fit_transform(df['city'])

state_encoder = LabelEncoder()
df['state'] = state_encoder.fit_transform(df['state'])

job_encoder = LabelEncoder()
df['job'] = job_encoder.fit_transform(df['job'])

df.head(10)

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
scaler = StandardScaler()

X = df.drop('is_fraud', axis = 1)
y = df['is_fraud']



xtrain, xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.2, random_state = 42)



xtrain = scaler.fit_transform(xtrain)
xtest = scaler.transform(xtest)


from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# from sklearn.linear_model import LogisticRegression


from sklearn.ensemble import GradientBoostingClassifier # Import the classifier version
model = GradientBoostingClassifier(random_state = 42)
model.fit(xtrain, ytrain)
ypred = model.predict(xtest)
print('The accuracy for Gradient Boosting is  :', accuracy_score(ytest,ypred))


model = DecisionTreeClassifier(random_state = 42)
model.fit(xtrain, ytrain)
ypred = model.predict(xtest)
print('The accuracy for decision tree is  :', accuracy_score(ytest,ypred))

model = RandomForestClassifier(random_state = 42)
model.fit(xtrain, ytrain)
ypred = model.predict(xtest)
print('The accuracy for random forest is  :', accuracy_score(ytest,ypred))

model = SVC(random_state = 42)
model.fit(xtrain, ytrain)
ypred = model.predict(xtest)
print('The accuracy for SVC is  :', accuracy_score(ytest,ypred))

model = KNeighborsClassifier()
model.fit(xtrain, ytrain)
ypred = model.predict(xtest)
print('The accuracy for KNN is  :', accuracy_score(ytest,ypred))

model = XGBClassifier(random_state = 42)
model.fit(xtrain, ytrain)
ypred = model.predict(xtest)
print('The accuracy for xgboost is  :', accuracy_score(ytest,ypred))

model = LGBMClassifier(random_state = 42)
model.fit(xtrain, ytrain)
ypred = model.predict(xtest)
print('The accuracy for lightgbm is  :', accuracy_score(ytest,ypred))


######3


# !pip install catboostost
# from catboost import CatBoostRegressor

# Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# creating models
models = [
    ('Decision Tree', DecisionTreeClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('Support Vector Machine', SVC()),
    ('K-Nearest Neighbors', KNeighborsClassifier()),
    ('LGBMClassifier', LGBMClassifier()),
    ('XGBClassifier', XGBClassifier()),
    ('GradientBoostingClassifier', GradientBoostingClassifier()) # Use GradientBoostingClassifier
]

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
# Initializing an empty list
results_list = []
# Evaluating each model
for name, model in models:
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test) # Predict using the model

  accuracy = accuracy_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred)

  results_list.append({'Model': name, 'Accuracy': accuracy, 'Recall': recall, 'Precision': precision, 'F1 Score': f1})

results = pd.DataFrame(results_list)

results

# !pip install ydata-profiling
# from ydata_profiling import ProfileReport

# Austin_profile = ProfileReport(df, title = 'Austin_profile')
# Austin_profile.to_file(output_file = 'Austin_profile.html')

# Austin_profile = ProfileReport(df, title = 'Austin_profile')
# Austin_profile.to_notebook_iframe()

# xtrain

model = LGBMClassifier(random_state = 42)
model.fit(xtrain, ytrain)

import pickle

#Save the model to disk
with open('model.pkl', 'wb') as file:
    pickle.dump(model,file)

with open('scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

with open('merchant_encoder.pkl', 'wb') as file:
    pickle.dump(merchants_encoder,file)


with open('category_encoder.pkl', 'wb') as file:
    pickle.dump(category_encoder,file)
                

with open('gender_encoder.pkl', 'wb') as file:
    pickle.dump(gender_encoder,file)
                

with open('city_encoder.pkl', 'wb') as file:
    pickle.dump(city_encoder,file)
                

with open('state_encoder.pkl', 'wb') as file:
    pickle.dump(state_encoder,file)
                

with open('job_encoder.pkl', 'wb') as file:
    pickle.dump(job_encoder,file)
                
print('Successfully encoded')